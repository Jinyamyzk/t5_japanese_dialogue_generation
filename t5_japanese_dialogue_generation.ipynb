{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "t5_japanese_dialogue_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZJQh4NBaR28"
      },
      "source": [
        "## 依存ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPD9u-fzVWhr"
      },
      "source": [
        "!pip install -qU torch==1.7.1 torchtext==0.8.0 torchvision==0.8.2\n",
        "!pip install -q transformers==4.4.2 pytorch_lightning==1.2.1 sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLuhyVmLaaHp"
      },
      "source": [
        "## 各種ディレクトリ作成\n",
        "\n",
        "\n",
        "*   data: 学習用データセット格納用\n",
        "*   model: 学習済モデル格納用\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n24ggrsAVomv"
      },
      "source": [
        "!mkdir -p /content/data /content/model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB-auw3TWnMz"
      },
      "source": [
        "# 事前学習済みモデル\n",
        "PRETRAINED_MODEL_NAME = \"sonoisa/t5-base-japanese\"\n",
        "\n",
        "# 転移学習済みモデル\n",
        "MODEL_DIR = \"/content/model\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACYGyC9SWpwm"
      },
      "source": [
        "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def unicode_normalize(cls, s):\n",
        "    pt = re.compile('([{}]+)'.format(cls))\n",
        "\n",
        "    def norm(c):\n",
        "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
        "\n",
        "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
        "    s = re.sub('－', '-', s)\n",
        "    return s\n",
        "\n",
        "def remove_extra_spaces(s):\n",
        "    s = re.sub('[ 　]+', ' ', s)\n",
        "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
        "                      '\\u3040-\\u309F',  # HIRAGANA\n",
        "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
        "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
        "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
        "                      ))\n",
        "    basic_latin = '\\u0000-\\u007F'\n",
        "\n",
        "    def remove_space_between(cls1, cls2, s):\n",
        "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
        "        while p.search(s):\n",
        "            s = p.sub(r'\\1\\2', s)\n",
        "        return s\n",
        "\n",
        "    s = remove_space_between(blocks, blocks, s)\n",
        "    s = remove_space_between(blocks, basic_latin, s)\n",
        "    s = remove_space_between(basic_latin, blocks, s)\n",
        "    return s\n",
        "\n",
        "def normalize_neologd(s):\n",
        "    s = s.strip()\n",
        "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
        "\n",
        "    def maketrans(f, t):\n",
        "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
        "\n",
        "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
        "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
        "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
        "    s = s.translate(\n",
        "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
        "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
        "\n",
        "    s = remove_extra_spaces(s)\n",
        "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
        "    s = re.sub('[’]', '\\'', s)\n",
        "    s = re.sub('[”]', '\"', s)\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2OZ3xo9XHz-"
      },
      "source": [
        "import re\n",
        "import csv\n",
        "\n",
        "\n",
        "def remove_brackets(text):\n",
        "    text = re.sub(r\"(^【[^】]*】)|(【[^】]*】$)\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def normalize_text(text):\n",
        "    assert \"\\n\" not in text and \"\\r\" not in text\n",
        "    text = text.encode().decode(\"utf-8\").strip()\n",
        "    text = text.replace(\"\\t\", \" \")\n",
        "    text = text.strip()\n",
        "    text = normalize_neologd(text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def read_title_body(file):\n",
        "    next(file)\n",
        "    next(file)\n",
        "    title = next(file).decode(\"utf-8\").strip()\n",
        "    title = normalize_text(remove_brackets(title))\n",
        "    body = normalize_text(\" \".join([line.decode(\"utf-8\").strip() for line in file.readlines()]))\n",
        "    return title, body\n",
        "  \n",
        "all_data = []\n",
        "\n",
        "with open(\"./nucc_dataset.tsv\") as f:\n",
        "  reader = csv.reader(f, delimiter='\\t')\n",
        "  for row in reader:\n",
        "    utterance = row[0]\n",
        "    response = row[1]\n",
        "    utterance = normalize_text(utterance)\n",
        "    response = normalize_text(response)\n",
        "\n",
        "    if len(utterance) > 0 and len(response) > 0:\n",
        "      all_data.append({\n",
        "          \"utterance\": utterance,\n",
        "          \"response\": response\n",
        "      })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2pA3x6-gWPj"
      },
      "source": [
        "## データ分割\n",
        "\n",
        "データセットを90% : 5%: 5% の比率でtrain/dev/testに分割します。\n",
        "\n",
        "* trainデータ: 学習に利用するデータ\n",
        "* devデータ: 学習中の精度評価等に利用するデータ\n",
        "* testデータ: 学習結果のモデルの精度評価に利用するデータ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSHmIQxoc6Zm"
      },
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "random.seed(1234)\n",
        "random.shuffle(all_data)\n",
        "\n",
        "def to_line(data):\n",
        "    utterance = data[\"utterance\"]\n",
        "    response = data[\"response\"]\n",
        "\n",
        "    assert len(utterance) > 0 and len(response) > 0\n",
        "    return f\"{utterance}\\t{response}\\n\"\n",
        "\n",
        "data_size = len(all_data)\n",
        "train_ratio, dev_ratio, test_ratio = 0.9, 0.05, 0.05\n",
        "\n",
        "with open(f\"data/train.tsv\", \"w\", encoding=\"utf-8\") as f_train, \\\n",
        "    open(f\"data/dev.tsv\", \"w\", encoding=\"utf-8\") as f_dev, \\\n",
        "    open(f\"data/test.tsv\", \"w\", encoding=\"utf-8\") as f_test:\n",
        "    \n",
        "    for i, data in tqdm(enumerate(all_data)):\n",
        "        line = to_line(data)\n",
        "        if i < train_ratio * data_size:\n",
        "            f_train.write(line)\n",
        "        elif i < (train_ratio + dev_ratio) * data_size:\n",
        "            f_dev.write(line)\n",
        "        else:\n",
        "            f_test.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyWz8b9AicYz"
      },
      "source": [
        "作成されたデータを確認します。  \n",
        "形式: {発話}\\t{応答}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6w5EvF5d3n-"
      },
      "source": [
        "!head -3 data/test.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSHVkqz6iyHW"
      },
      "source": [
        "## 学習に必要なクラス等の定義\n",
        "学習にはPyTorch/PyTorch-lightning/Transformersを利用します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ijEcsSOi-hy"
      },
      "source": [
        "import argparse\n",
        "import glob\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "import random\n",
        "import re\n",
        "from itertools import chain\n",
        "from string import punctuation\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "\n",
        "# 乱数シードの設定\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHgBwuNFjGKo"
      },
      "source": [
        "# GPU利用有無\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "\n",
        "# 各種ハイパーパラメータ\n",
        "args_dict = dict(\n",
        "    data_dir=\"/content/data\",  # データセットのディレクトリ\n",
        "    model_name_or_path=PRETRAINED_MODEL_NAME,\n",
        "    tokenizer_name_or_path=PRETRAINED_MODEL_NAME,\n",
        "\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.0,\n",
        "    adam_epsilon=1e-8,\n",
        "    warmup_steps=0,\n",
        "    gradient_accumulation_steps=1,\n",
        "\n",
        "    # max_input_length=64,\n",
        "    # max_target_length=512,\n",
        "    # train_batch_size=8,\n",
        "    # eval_batch_size=8,\n",
        "    # num_train_epochs=10,\n",
        "\n",
        "    n_gpu=1 if USE_GPU else 0,\n",
        "    early_stop_callback=False,\n",
        "    fp_16=False,\n",
        "    opt_level='O1',\n",
        "    max_grad_norm=1.0,\n",
        "    seed=42,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qReZ2yBhkL03"
      },
      "source": [
        "## TSVデータセットクラス\n",
        "\n",
        "TSV形式のファイルをデータセットとして読み込みます。  \n",
        "形式は\"{utterance}\\t{response}\"です"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB2trpg2jYNJ"
      },
      "source": [
        "class TsvDataset(Dataset):\n",
        "    def __init__(self, tokenizer, data_dir, type_path, input_max_len=512, target_max_len=512):\n",
        "        self.file_path = os.path.join(data_dir, type_path)\n",
        "        \n",
        "        self.input_max_len = input_max_len\n",
        "        self.target_max_len = target_max_len\n",
        "        self.tokenizer = tokenizer\n",
        "        self.inputs = []\n",
        "        self.targets = []\n",
        "\n",
        "        self._build()\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "  \n",
        "    def __getitem__(self, index):\n",
        "        source_ids = self.inputs[index][\"input_ids\"].squeeze()\n",
        "        target_ids = self.targets[index][\"input_ids\"].squeeze()\n",
        "\n",
        "        source_mask = self.inputs[index][\"attention_mask\"].squeeze()\n",
        "        target_mask = self.targets[index][\"attention_mask\"].squeeze()\n",
        "\n",
        "        return {\"source_ids\": source_ids, \"source_mask\": source_mask, \n",
        "                \"target_ids\": target_ids, \"target_mask\": target_mask}\n",
        "\n",
        "    def _make_record(self, utterance, response):\n",
        "        # 応答生成タスク用の入出力形式に変換する。\n",
        "        input = f\"{utterance}\"\n",
        "        target = f\"{response}\"\n",
        "        return input, target\n",
        "  \n",
        "    def _build(self):\n",
        "        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip().split(\"\\t\")\n",
        "                assert len(line) == 2\n",
        "                assert len(line[0]) > 0\n",
        "                assert len(line[1]) > 0\n",
        "\n",
        "                utterance = line[0]\n",
        "                response = line[1]\n",
        "\n",
        "                input, target = self._make_record(utterance, response)\n",
        "\n",
        "                tokenized_inputs = self.tokenizer.batch_encode_plus(\n",
        "                    [input], max_length=self.input_max_len, truncation=True, \n",
        "                    padding=\"max_length\", return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                tokenized_targets = self.tokenizer.batch_encode_plus(\n",
        "                    [target], max_length=self.target_max_len, truncation=True, \n",
        "                    padding=\"max_length\", return_tensors=\"pt\"\n",
        "                )\n",
        "\n",
        "                self.inputs.append(tokenized_inputs)\n",
        "                self.targets.append(tokenized_targets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTHutL-Gk45C"
      },
      "source": [
        "試しにテストデータ（test.tsv）を読み込み、トークナイズ結果をみてみます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAPH1_yMk1fe"
      },
      "source": [
        "# トークナイザー（SentencePiece）モデルの読み込み\n",
        "tokenizer = T5Tokenizer.from_pretrained(PRETRAINED_MODEL_NAME, is_fast=True)\n",
        "\n",
        "# テストデータセットの読み込み\n",
        "test_dataset = TsvDataset(tokenizer, args_dict[\"data_dir\"], \"test.tsv\", \n",
        "                           input_max_len=128, target_max_len=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_a3n6qEfljsq"
      },
      "source": [
        "テストデータの1レコード目をみてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BdtrnW7k-vh"
      },
      "source": [
        "for data in test_dataset:\n",
        "    print(\"A. 入力データの元になる文字列\")\n",
        "    print(tokenizer.decode(data[\"source_ids\"]))\n",
        "    print()\n",
        "    print(\"B. 入力データ（Aの文字列がトークナイズされたトークンID列）\")\n",
        "    print(data[\"source_ids\"])\n",
        "    print()\n",
        "    print(\"C. 出力データの元になる文字列\")\n",
        "    print(tokenizer.decode(data[\"target_ids\"]))\n",
        "    print()\n",
        "    print(\"D. 出力データ（Cの文字列がトークナイズされたトークンID列）\")\n",
        "    print(data[\"target_ids\"])\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqojMCHplp-5"
      },
      "source": [
        "## 学習処理クラス\n",
        "\n",
        "PyTorch-Lightningを使って学習します。\n",
        "\n",
        "PyTorch-Lightningとは、機械学習の典型的な処理を簡潔に書くことができるフレームワークです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6u78ackXlmV4"
      },
      "source": [
        "class T5FineTuner(pl.LightningModule):\n",
        "    def __init__(self, hparams):\n",
        "        super().__init__()\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # 事前学習済みモデルの読み込み\n",
        "        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n",
        "\n",
        "        # トークナイザーの読み込み\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path, is_fast=True)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, \n",
        "                decoder_attention_mask=None, labels=None):\n",
        "        \"\"\"順伝搬\"\"\"\n",
        "        return self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "    def _step(self, batch):\n",
        "        \"\"\"ロス計算\"\"\"\n",
        "        labels = batch[\"target_ids\"]\n",
        "\n",
        "        # All labels set to -100 are ignored (masked), \n",
        "        # the loss is only computed for labels in [0, ..., config.vocab_size]\n",
        "        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        outputs = self(\n",
        "            input_ids=batch[\"source_ids\"],\n",
        "            attention_mask=batch[\"source_mask\"],\n",
        "            decoder_attention_mask=batch['target_mask'],\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs[0]\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        \"\"\"訓練ステップ処理\"\"\"\n",
        "        loss = self._step(batch)\n",
        "        self.log(\"train_loss\", loss)\n",
        "        return {\"loss\": loss}\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        \"\"\"バリデーションステップ処理\"\"\"\n",
        "        loss = self._step(batch)\n",
        "        self.log(\"val_loss\", loss)\n",
        "        return {\"val_loss\": loss}\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        \"\"\"テストステップ処理\"\"\"\n",
        "        loss = self._step(batch)\n",
        "        self.log(\"test_loss\", loss)\n",
        "        return {\"test_loss\": loss}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"オプティマイザーとスケジューラーを作成する\"\"\"\n",
        "        model = self.model\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() \n",
        "                            if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": self.hparams.weight_decay,\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters() \n",
        "                            if any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_grouped_parameters, \n",
        "                          lr=self.hparams.learning_rate, \n",
        "                          eps=self.hparams.adam_epsilon)\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, num_warmup_steps=self.hparams.warmup_steps, \n",
        "            num_training_steps=self.t_total\n",
        "        )\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n",
        "\n",
        "    def get_dataset(self, tokenizer, type_path, args):\n",
        "        \"\"\"データセットを作成する\"\"\"\n",
        "        return TsvDataset(\n",
        "            tokenizer=tokenizer, \n",
        "            data_dir=args.data_dir, \n",
        "            type_path=type_path, \n",
        "            input_max_len=args.max_input_length,\n",
        "            target_max_len=args.max_target_length)\n",
        "    \n",
        "    def setup(self, stage=None):\n",
        "        \"\"\"初期設定（データセットの読み込み）\"\"\"\n",
        "        if stage == 'fit' or stage is None:\n",
        "            train_dataset = self.get_dataset(tokenizer=self.tokenizer, \n",
        "                                             type_path=\"train.tsv\", args=self.hparams)\n",
        "            self.train_dataset = train_dataset\n",
        "\n",
        "            val_dataset = self.get_dataset(tokenizer=self.tokenizer, \n",
        "                                           type_path=\"dev.tsv\", args=self.hparams)\n",
        "            self.val_dataset = val_dataset\n",
        "\n",
        "            self.t_total = (\n",
        "                (len(train_dataset) // (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n",
        "                // self.hparams.gradient_accumulation_steps\n",
        "                * float(self.hparams.num_train_epochs)\n",
        "            )\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"訓練データローダーを作成する\"\"\"\n",
        "        return DataLoader(self.train_dataset, \n",
        "                          batch_size=self.hparams.train_batch_size, \n",
        "                          drop_last=True, shuffle=True, num_workers=4)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        \"\"\"バリデーションデータローダーを作成する\"\"\"\n",
        "        return DataLoader(self.val_dataset, \n",
        "                          batch_size=self.hparams.eval_batch_size, \n",
        "                          num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GimgEqImcgE"
      },
      "source": [
        "## 転移学習を実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-z-18Q1mYwY"
      },
      "source": [
        "# 学習に用いるハイパーパラメータを設定する\n",
        "args_dict.update({\n",
        "    \"max_input_length\":  128,  # 入力文の最大トークン数\n",
        "    \"max_target_length\": 128,  # 出力文の最大トークン数\n",
        "    \"train_batch_size\":  8,\n",
        "    \"eval_batch_size\":   8,\n",
        "    \"num_train_epochs\":  10\n",
        "    })\n",
        "args = argparse.Namespace(**args_dict)\n",
        "\n",
        "train_params = dict(\n",
        "    accumulate_grad_batches=args.gradient_accumulation_steps,\n",
        "    gpus=args.n_gpu,\n",
        "    max_epochs=args.num_train_epochs,\n",
        "    precision= 16 if args.fp_16 else 32,\n",
        "    amp_level=args.opt_level,\n",
        "    gradient_clip_val=args.max_grad_norm,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKFsn3wemjPb"
      },
      "source": [
        "# 転移学習の実行（GPUを利用すれば1エポック10分程度）\n",
        "model = T5FineTuner(args)\n",
        "trainer = pl.Trainer(**train_params)\n",
        "trainer.fit(model)\n",
        "\n",
        "# 最終エポックのモデルを保存\n",
        "model.tokenizer.save_pretrained(MODEL_DIR)\n",
        "model.model.save_pretrained(MODEL_DIR)\n",
        "\n",
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TSLlW9Kml3p"
      },
      "source": [
        "# トークナイザー（SentencePiece）\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_DIR, is_fast=True)\n",
        "\n",
        "# 学習済みモデル\n",
        "trained_model = T5ForConditionalGeneration.from_pretrained(MODEL_DIR)\n",
        "\n",
        "# GPUの利用有無\n",
        "USE_GPU = torch.cuda.is_available()\n",
        "if USE_GPU:\n",
        "    trained_model.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpUg3AKJXp56"
      },
      "source": [
        "import textwrap\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn import metrics\n",
        "\n",
        "# テストデータの読み込み\n",
        "test_dataset = TsvDataset(tokenizer, args_dict[\"data_dir\"], \"test.tsv\", \n",
        "                          input_max_len=args.max_input_length, \n",
        "                          target_max_len=args.max_target_length)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=8, num_workers=4)\n",
        "\n",
        "trained_model.eval()\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "targets = []\n",
        "\n",
        "for batch in tqdm(test_loader):\n",
        "    input_ids = batch['source_ids']\n",
        "    input_mask = batch['source_mask']\n",
        "    if USE_GPU:\n",
        "        input_ids = input_ids.cuda()\n",
        "        input_mask = input_mask.cuda()\n",
        "\n",
        "    output = trained_model.generate(input_ids=input_ids, \n",
        "        attention_mask=input_mask, \n",
        "        max_length=args.max_target_length,\n",
        "        repetition_penalty=10.0,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n",
        "        )\n",
        "\n",
        "    output_text = [tokenizer.decode(ids, skip_special_tokens=True, \n",
        "                            clean_up_tokenization_spaces=False) \n",
        "                for ids in output]\n",
        "    target_text = [tokenizer.decode(ids, skip_special_tokens=True, \n",
        "                               clean_up_tokenization_spaces=False) \n",
        "                for ids in batch[\"target_ids\"]]\n",
        "    input_text = [tokenizer.decode(ids, skip_special_tokens=True, \n",
        "                               clean_up_tokenization_spaces=False) \n",
        "                for ids in input_ids]\n",
        "\n",
        "    inputs.extend(input_text)\n",
        "    outputs.extend(output_text)\n",
        "    targets.extend(target_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSqVidMjYFfI"
      },
      "source": [
        "for output, target, input in zip(outputs, targets, inputs):\n",
        "    print(\"utterance:     \" + input)\n",
        "    print(\"generated response: \" + output)\n",
        "    print(\"actual:    \" + target)\n",
        "    print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DQJ_fkkb3N2"
      },
      "source": [
        "dir=\"./drive/MyDrive/t5_result.csv\"\n",
        "with open(dir,\"w\") as f:\n",
        "  writer = csv.writer(f)\n",
        "  writer.writerow([\"input\",\"generated response\",\"acutual response\"])\n",
        "  for output, target, input in zip(outputs, targets, inputs):\n",
        "    writer.writerow([input,output,target])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOl6EhH1ezF0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}